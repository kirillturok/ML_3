{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/QBunN0IIgH9M9poeqsXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirillturok/ML_3/blob/main/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "gkuJFbQfmUNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get notMNIST data"
      ],
      "metadata": {
        "id": "NEdYe7s7nU33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "dataset_url = \"https://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz\"\n",
        "dataset_dir = tf.keras.utils.get_file('notMNIST_large.tar', origin=dataset_url, extract=True)\n",
        "dataset_dir = pathlib.Path(dataset_dir).with_suffix('')\n"
      ],
      "metadata": {
        "id": "JvGGNBiz2DYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78ee910-cc29-4217-a5f2-79857701ad9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz\n",
            "247336696/247336696 [==============================] - 9s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataframe"
      ],
      "metadata": {
        "id": "UsdvUhItmgGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "CLASSES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
        "DATA_COLUMN = 'data'\n",
        "LABELS_COLUMN = 'labels'\n",
        "HASHED_DATA_COLUMN = 'hashed'\n",
        "\n",
        "def get_class_data(folder_path):\n",
        "    result_data = list()\n",
        "    files = os.listdir(folder_path)\n",
        "    for file in files:\n",
        "        image_path = os.path.join(folder_path, file)\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is not None:\n",
        "            result_data.append(img)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "def create_data_frame():\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    for class_item in CLASSES:\n",
        "        class_folder_path = os.path.join(dataset_dir, class_item)\n",
        "        class_data = get_class_data(class_folder_path)\n",
        "\n",
        "        data.extend(class_data)\n",
        "        labels.extend([CLASSES.index(class_item) for _ in range(len(class_data))])\n",
        "\n",
        "    data_frame = pd.DataFrame({DATA_COLUMN: data, LABELS_COLUMN: labels})\n",
        "\n",
        "    return data_frame\n",
        "\n",
        "data_frame = create_data_frame()\n"
      ],
      "metadata": {
        "id": "01tcr2VIepZ_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data"
      ],
      "metadata": {
        "id": "xZOQg_nln4z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(data):\n",
        "    data_bytes = [item.tobytes() for item in data[DATA_COLUMN]]\n",
        "    data[HASHED_DATA_COLUMN] = data_bytes\n",
        "    data.sort_values(HASHED_DATA_COLUMN, inplace=True)\n",
        "    data.drop_duplicates(subset=HASHED_DATA_COLUMN, keep='first', inplace=True)\n",
        "    data.pop(HASHED_DATA_COLUMN)\n",
        "\n",
        "    return data\n",
        "\n",
        "df_no_duplicates = remove_duplicates(data_frame)\n",
        "\n",
        "min_class_count = df_no_duplicates[LABELS_COLUMN].value_counts().min()\n",
        "balanced_df = pd.concat([df_no_duplicates[df_no_duplicates[LABELS_COLUMN] == label].sample(min_class_count) for label in df_no_duplicates[LABELS_COLUMN].unique()])\n",
        "\n",
        "df = balanced_df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "N1p0DFds2mjJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide into subsamples"
      ],
      "metadata": {
        "id": "eXoZ1vtConov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.color import rgb2gray\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def divide_into_subsamples(data_frame):\n",
        "    data = np.array(data_frame[DATA_COLUMN].values)\n",
        "    labels = np.array(data_frame[LABELS_COLUMN].values)\n",
        "\n",
        "    data_gray = np.array([rgb2gray(img) for img in data])\n",
        "    data_gray = data_gray.reshape(-1, 28, 28, 1)\n",
        "    data_gray = data_gray.astype('float32')\n",
        "\n",
        "    x_train, x_other, y_train, y_other = train_test_split(data_gray, labels, train_size=0.2, random_state = 10)\n",
        "    x_test, x_val, y_test, y_val = train_test_split(x_other, y_other, train_size = 0.5, random_state = 10)\n",
        "\n",
        "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, to_categorical(y_train, num_classes=10)))\n",
        "    dataset_test = tf.data.Dataset.from_tensor_slices((x_test, to_categorical(y_test, num_classes = 10)))\n",
        "    dataset_val = tf.data.Dataset.from_tensor_slices((x_val, to_categorical(y_val, num_classes = 10)))\n",
        "\n",
        "    return dataset_train, dataset_test, dataset_val\n",
        "\n",
        "dataset_train, dataset_test, dataset_val = divide_into_subsamples(df)\n",
        "\n",
        "dataset_train = dataset_train.batch(BATCH_SIZE)\n",
        "dataset_test = dataset_test.batch(BATCH_SIZE)\n",
        "dataset_val = dataset_val.batch(BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "2L_0IPu3sOzG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "NBwr02sQj_r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models declaration"
      ],
      "metadata": {
        "id": "vABwlLxMrGu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "input_shape = (28, 28)\n",
        "num_classes = len(CLASSES)\n",
        "\n",
        "# convolutional model\n",
        "conv_model = models.Sequential([\n",
        "    tf.keras.layers.Rescaling(1. / 255),\n",
        "\n",
        "    tf.keras.layers.Conv2D(\n",
        "        32, (3, 3), activation='relu',\n",
        "        input_shape = input_shape),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "conv_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Pooling model\n",
        "pooling_model = models.Sequential([\n",
        "    tf.keras.layers.Rescaling(1. / 255),\n",
        "\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), input_shape = input_shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "pooling_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# LeNet-5\n",
        "lenet_model = models.Sequential([\n",
        "    tf.keras.layers.Rescaling(1. / 255),\n",
        "\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape = input_shape),\n",
        "    layers.AveragePooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.AveragePooling2D((2, 2)),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "lenet_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9iXQkgYZsJn9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters initialization"
      ],
      "metadata": {
        "id": "I-juFhzkuxZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "JyFJ6Em0u72B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional model processing"
      ],
      "metadata": {
        "id": "ilZaH5XRrMXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model.fit(\n",
        "    dataset_train,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = dataset_val)\n",
        "\n",
        "test_loss, test_acc = conv_model.evaluate(dataset_val)\n",
        "print(f'\\nConvolutional Model\\n\\tTest Accuracy: {test_acc}\\n\\tTest Loss: {test_loss}')\n"
      ],
      "metadata": {
        "id": "ge7NvFOfq_BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pooling model processing"
      ],
      "metadata": {
        "id": "MVSqdm1jwDAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooling_model.fit(\n",
        "    dataset_train,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = dataset_val)\n",
        "\n",
        "test_loss, test_acc = pooling_model.evaluate(dataset_val)\n",
        "print(f'\\nPooling Model\\n\\tTest Accuracy: {test_acc}\\n\\tTest Loss: {test_loss}')"
      ],
      "metadata": {
        "id": "B9oz2t9ZwCnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 Model processing"
      ],
      "metadata": {
        "id": "VKmeLUf2z9iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.fit(\n",
        "    dataset_train,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = dataset_val)\n",
        "\n",
        "test_loss, test_acc = lenet_model.evaluate(dataset_val)\n",
        "print(f'\\nLeNet-5 Model\\n\\tTest Accuracy: {test_acc}\\n\\tTest Loss: {test_loss}')"
      ],
      "metadata": {
        "id": "8V17Ax860Aoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}